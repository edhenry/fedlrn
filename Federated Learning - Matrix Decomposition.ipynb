{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import toimage\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import io\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning : Improving comms notes\n",
    "\n",
    "These notes are used to track the methods that are used within the paper : \n",
    "\n",
    "\n",
    "[FEDERATED LEARNING: STRATEGIES FOR IMPROVING COMMUNICATION EFFICIENCY](http://arxiv.org/abs/1610.05492)\n",
    "\n",
    "This paper outlines how to improve network communications in a federated learnig environment. The parameter updates that are transimitted, per device, back to a centralized parameter server can decrease overall system throughput substantially. We will use this notebook as a way to explore some of the transformation methodologies used.\n",
    "\n",
    "## Notation\n",
    "\n",
    "$W$ - initial model parameters that will be transmitted to the hosts\n",
    "\n",
    "$\\mathbb{R}^{d_{1} x d_{2}}$ - matrix of real numbers where $d_{1}$ and $d_{2}$ represent the model input dimensions and output dimensions, respectively.\n",
    "\n",
    "$t$ - integer tracking the round, or number of iterations, this process has been performed\n",
    "\n",
    "$W_{t}$ - initial (or current) model \n",
    "\n",
    "$S_{t}$ - set of clients available where we can distribute the model to\n",
    "\n",
    "$n_{t}$ - integer tracking the number of nodes participating in this process\n",
    "\n",
    "$W_{t}^{1}, W_{t}^{2}, \\ldots , W_{t}^{n_{t}}$ - set of updates local models at each time step over each set of nodes\n",
    "\n",
    "$H_{t}^{i} := W{t}^{i} - W_{t}, \\text{for} \\ i \\in S_{t}$ - the update of a client at a given time step. The difference between parameters at $(t+1) - t$\n",
    "\n",
    "$\\eta$ - centralized learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The parameters that are assumed within this model are defined as\n",
    "\n",
    "$W \\in \\mathbb{R}^{d_1 x d_2}$\n",
    "\n",
    "This parameter distribution, training, and update process is iterative. We will assume that $t = 0$ is the first round of this process.\n",
    "\n",
    "At time $t \\ge 0$ the server will distribute the current model parameters $W_{t}$ to a subset $S_{t}$ of $n_{t}$ clients.\n",
    "\n",
    "The update cycles can be single steps of multiple steps of SGD taken over a client's local datasets. These updates are sent back to the server and a global update ic computed as :\n",
    "\n",
    "$$ W_{t+1} = W_{t} + \\eta_{t}H_{t}, \\ \\ \\ H_{t} := \\frac{1}{n_{t}} \\sum_{i \\in S_{t}} H_{t}^{i} $$\n",
    "\n",
    "In words, we can see that the updated parameter matrix is a sum of the original parameter space plus the summed updates of the clients that consists of the difference of the previous parameter matrix and the new, updated, parameter matrix.\n",
    "\n",
    "The idea is the minimize the amount of communcation overhead by representing $H_{t}^{i}$. \n",
    "\n",
    "We will now review the two approaches outline within the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Update\n",
    "\n",
    "In this case we're going to pre-specify the structure of $H_{t}^{i}$.\n",
    "\n",
    "### Low Rank \n",
    "\n",
    "Here we enforce a bounding on $H_{t}^{i} \\in \\mathbb{R}^{d_{1} x d_{2}}$ to be of low rank $k$, where $k$ is a fixed number.\n",
    "\n",
    "This requires that we express $H_{t}^{i}$ as a product of two matrices.\n",
    "\n",
    "$$ H_{t}^{i} = A_{t}^{i}B_{t}^{i} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ A_{t}^{i} \\in \\mathbb{R}^{d_{1} x k}, B_{t}^{i} \\in \\mathbb{R}^{k x d_{2}} $$\n",
    "\n",
    "Here we can see the bounding of the rank K over the column space and row space of the respective matrices defined above.\n",
    "\n",
    "### Random Mask\n",
    "\n",
    "Random masking is a bit different in that the authors are trying to restrict $H_{t}^{i}$ to be sparse. They take the approach of generating the sparse \"mask\" every each round of training for each client, independently.\n",
    "\n",
    "Once they've generated and applied this mask to $H_{t}^{i}$ we can then only transmit the non-zero entries of the matrix back to the centralized parameter server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37273288 0.37044459 0.65182003 0.10166379 0.06097416]\n",
      " [0.14750606 0.88525255 0.47471904 0.90438684 0.85763023]\n",
      " [0.28702413 0.53754398 0.17847662 0.80663598 0.97645826]\n",
      " [0.54271045 0.10274996 0.47252215 0.47035657 0.98372558]\n",
      " [0.31032767 0.59428442 0.94144556 0.41962019 0.8778428 ]]\n",
      "[[0.37273288 0.37044459 0.65182003 0.10166379 0.06097416]\n",
      " [0.14750606 0.         0.47471904 0.90438684 0.        ]\n",
      " [0.         0.         0.17847662 0.80663598 0.97645826]\n",
      " [0.54271045 0.         0.47252215 0.47035657 0.98372558]\n",
      " [0.         0.         0.94144556 0.41962019 0.        ]]\n",
      "[[0.8216995  0.64327215 0.2636716  0.91463054 0.39264698]\n",
      " [0.43795556 0.08148498 0.01706277 0.20059502 0.1302347 ]\n",
      " [0.76815752 0.94630677 0.92258022 0.31050166 0.50240623]\n",
      " [0.76124367 0.68396606 0.10369729 0.72925216 0.19375556]\n",
      " [0.02756239 0.61009921 0.11715617 0.95808076 0.58471186]]\n",
      "[[0.         0.64327215 0.         0.         0.39264698]\n",
      " [0.43795556 0.         0.         0.20059502 0.        ]\n",
      " [0.         0.         0.92258022 0.31050166 0.50240623]\n",
      " [0.76124367 0.         0.         0.72925216 0.        ]\n",
      " [0.02756239 0.61009921 0.11715617 0.         0.58471186]]\n",
      "[[0.3686419  0.61603254 0.53188415 0.68562695 0.55383537]\n",
      " [0.1303822  0.12695721 0.13856476 0.45072204 0.15172083]\n",
      " [0.44443053 0.65704808 0.6785615  0.08994264 0.64841525]\n",
      " [0.36749691 0.49558066 0.86950119 0.39624114 0.194534  ]\n",
      " [0.84359891 0.48500877 0.96485893 0.05807358 0.89944239]]\n",
      "[[0.3686419  0.         0.53188415 0.         0.55383537]\n",
      " [0.1303822  0.12695721 0.         0.45072204 0.15172083]\n",
      " [0.         0.         0.6785615  0.08994264 0.64841525]\n",
      " [0.         0.         0.86950119 0.39624114 0.194534  ]\n",
      " [0.         0.         0.         0.         0.89944239]]\n",
      "[[0.85180249 0.7050866  0.97457924 0.01293162 0.90435425]\n",
      " [0.91356679 0.05261984 0.29638229 0.48246209 0.39237866]\n",
      " [0.16689932 0.74725247 0.81428042 0.48528613 0.24627783]\n",
      " [0.93259514 0.10987098 0.65904451 0.61105018 0.10343212]\n",
      " [0.3519404  0.60835239 0.33819081 0.86346032 0.18746871]]\n",
      "[[0.85180249 0.7050866  0.97457924 0.         0.90435425]\n",
      " [0.91356679 0.         0.         0.48246209 0.        ]\n",
      " [0.         0.74725247 0.         0.         0.        ]\n",
      " [0.         0.10987098 0.         0.         0.        ]\n",
      " [0.3519404  0.60835239 0.33819081 0.86346032 0.        ]]\n",
      "[[7.47549541e-01 4.76955098e-01 1.91368987e-01 7.04722088e-01\n",
      "  1.90817652e-01]\n",
      " [3.73982392e-01 7.07317687e-01 4.90458320e-01 9.69641779e-01\n",
      "  5.59247237e-01]\n",
      " [2.01457022e-01 8.13591355e-01 2.15418483e-01 2.15186250e-01\n",
      "  3.96893879e-01]\n",
      " [6.22160658e-01 5.51353301e-01 5.67807265e-01 6.14082834e-01\n",
      "  3.76399173e-02]\n",
      " [2.42980077e-01 3.38625234e-04 5.37995612e-01 8.56910575e-01\n",
      "  2.68653064e-01]]\n",
      "[[7.47549541e-01 4.76955098e-01 0.00000000e+00 0.00000000e+00\n",
      "  1.90817652e-01]\n",
      " [3.73982392e-01 7.07317687e-01 4.90458320e-01 9.69641779e-01\n",
      "  5.59247237e-01]\n",
      " [0.00000000e+00 8.13591355e-01 2.15418483e-01 2.15186250e-01\n",
      "  3.96893879e-01]\n",
      " [0.00000000e+00 0.00000000e+00 5.67807265e-01 6.14082834e-01\n",
      "  3.76399173e-02]\n",
      " [0.00000000e+00 3.38625234e-04 5.37995612e-01 8.56910575e-01\n",
      "  2.68653064e-01]]\n",
      "[[0.44746072 0.701385   0.28366413 0.04851342 0.35402186]\n",
      " [0.64694093 0.47972956 0.00408033 0.34126726 0.56582222]\n",
      " [0.54498638 0.91046642 0.34644231 0.65000386 0.68744426]\n",
      " [0.67518556 0.44345889 0.38372371 0.98463473 0.33111942]\n",
      " [0.70549243 0.09082402 0.07565278 0.1638325  0.44241133]]\n",
      "[[0.         0.701385   0.         0.         0.35402186]\n",
      " [0.64694093 0.47972956 0.00408033 0.         0.        ]\n",
      " [0.54498638 0.91046642 0.34644231 0.         0.68744426]\n",
      " [0.67518556 0.         0.38372371 0.         0.33111942]\n",
      " [0.         0.09082402 0.07565278 0.         0.        ]]\n",
      "[[0.10760059 0.76538682 0.2257714  0.21204479 0.46611234]\n",
      " [0.78974906 0.17412867 0.58206103 0.69215839 0.73459641]\n",
      " [0.36487116 0.64675835 0.63863446 0.15691724 0.0324143 ]\n",
      " [0.06121731 0.22785416 0.89611701 0.58689235 0.7811505 ]\n",
      " [0.44253018 0.91769238 0.00827021 0.49036084 0.03842025]]\n",
      "[[0.         0.76538682 0.2257714  0.21204479 0.46611234]\n",
      " [0.78974906 0.17412867 0.         0.69215839 0.        ]\n",
      " [0.36487116 0.         0.63863446 0.15691724 0.0324143 ]\n",
      " [0.         0.22785416 0.         0.58689235 0.7811505 ]\n",
      " [0.         0.91769238 0.         0.49036084 0.        ]]\n",
      "[[0.29657021 0.54536104 0.44041972 0.01899648 0.70289108]\n",
      " [0.69248758 0.69866153 0.88376422 0.94470106 0.60016028]\n",
      " [0.2717245  0.41642209 0.00380079 0.26083505 0.40036934]\n",
      " [0.51154511 0.15394158 0.37501714 0.63933204 0.21621167]\n",
      " [0.8414545  0.71601553 0.24204738 0.67492673 0.82989622]]\n",
      "[[0.         0.54536104 0.         0.         0.        ]\n",
      " [0.         0.         0.88376422 0.94470106 0.        ]\n",
      " [0.         0.41642209 0.00380079 0.26083505 0.40036934]\n",
      " [0.         0.15394158 0.37501714 0.         0.        ]\n",
      " [0.         0.         0.24204738 0.67492673 0.        ]]\n",
      "[[0.76425311 0.45674013 0.03108498 0.41977882 0.28644521]\n",
      " [0.09851032 0.49361807 0.4573851  0.43022027 0.24732967]\n",
      " [0.25248862 0.00549337 0.66496123 0.89323019 0.06481329]\n",
      " [0.43282252 0.50618207 0.08638967 0.35304938 0.44538493]\n",
      " [0.17960702 0.96633629 0.59526877 0.80639138 0.50051056]]\n",
      "[[0.76425311 0.         0.03108498 0.41977882 0.        ]\n",
      " [0.         0.49361807 0.4573851  0.43022027 0.        ]\n",
      " [0.25248862 0.         0.66496123 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.17960702 0.         0.         0.         0.50051056]]\n",
      "[[0.59929085 0.38986574 0.04172468 0.25512608 0.39973933]\n",
      " [0.83161835 0.52654732 0.3965605  0.29188705 0.98295482]\n",
      " [0.02761433 0.14916307 0.30241867 0.70058534 0.23814666]\n",
      " [0.04992493 0.39696834 0.09015495 0.11366794 0.18745978]\n",
      " [0.84924668 0.95774828 0.00346214 0.27675094 0.20220236]]\n",
      "[[0.59929085 0.         0.         0.         0.39973933]\n",
      " [0.         0.52654732 0.3965605  0.         0.98295482]\n",
      " [0.         0.14916307 0.         0.70058534 0.23814666]\n",
      " [0.         0.39696834 0.09015495 0.         0.        ]\n",
      " [0.         0.95774828 0.00346214 0.27675094 0.20220236]]\n"
     ]
    }
   ],
   "source": [
    "# random mask example\n",
    "\n",
    "for i in range(10):\n",
    "    rand_mat = np.random.rand(5,5)\n",
    "    print(rand_mat)\n",
    "\n",
    "    mask = np.random.choice([True, False], size=(5,5))\n",
    "\n",
    "    print(rand_mat * mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_names():\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "        # note the encoding type is 'latin1'\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "        \n",
    "    return features, labels\n",
    "\n",
    "def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n",
    "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
    "    \n",
    "    if not (0 <= sample_id < len(features)):\n",
    "        print('{} samples in batch {}.  {} is out of range.'.format(len(features), batch_id, sample_id))\n",
    "        return None\n",
    "\n",
    "    print('\\nStats of batch #{}:'.format(batch_id))\n",
    "    print('# of Samples: {}\\n'.format(len(features)))\n",
    "    \n",
    "    label_names = load_label_names()\n",
    "    label_counts = dict(zip(*np.unique(labels, return_counts=True)))\n",
    "    for key, value in label_counts.items():\n",
    "        print('Label Counts of [{}]({}) : {}'.format(key, label_names[key].upper(), value))\n",
    "    \n",
    "    sample_image = features[sample_id]\n",
    "    sample_label = labels[sample_id]\n",
    "    \n",
    "    print('\\nExample of Image {}:'.format(sample_id))\n",
    "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
    "    print('Image - Shape: {}'.format(sample_image.shape))\n",
    "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
    "    \n",
    "    plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = load_cfar10_batch('/home/ed/Documents/code/fed_lrn/cifar-10-batches-py', 1)\n",
    "label_names = load_label_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb294d3db70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(features[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph - removing all existing variables, graph defns, etc.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "\n",
    "# Labels\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "\n",
    "# Probability that will be used for tf.nn.dropout()\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# sample conv net that we can use to model the matrix manipulations outlined above.\n",
    "def simple_conv(input_data, keep_prob):\n",
    "    \"\"\"\n",
    "    Simple convolutional network\n",
    "    \"\"\"\n",
    "    \n",
    "    # conv filters\n",
    "    conv_filter_1 = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    conv_filter_2 = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "    conv_filter_3 = tf.Variable(tf.truncated_normal(shape=[3, 3, 128, 256], mean=0, stddev=0.08))\n",
    "    conv_filter_4 = tf.Variable(tf.truncated_normal(shape=[3, 3, 256, 512], mean=0, stddev=0.08))\n",
    "\n",
    "    # layer 1 -> 2\n",
    "    conv1 = tf.nn.conv2d(input_data, conv_filter_1, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "    \n",
    "    # layer 2 -> 3\n",
    "    conv2 = tf.nn.conv2d(conv1_bn, conv_filter_2, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "    \n",
    "    # layer 3 -> 4\n",
    "    conv3 = tf.nn.conv2d(conv2_bn, conv_filter_3, strides=[1,1,1,1], padding='SAME')\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    conv3_pool = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv3_bn = tf.layers.batch_normalization(conv3_pool)\n",
    "    \n",
    "    # flatten output\n",
    "    flatten = tf.contrib.layers.flatten(conv3_bn)\n",
    "    \n",
    "    # layer 4 -> layer 5 (fully connected)\n",
    "    fc1 = tf.contrib.layers.fully_connected(inputs=flatten, num_outputs=128, activation_fn=tf.nn.relu)  \n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    fc1 = tf.layers.batch_normalization(fc1)\n",
    "    \n",
    "    # layer 5 -> 6\n",
    "    fc2 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    fc2 = tf.layers.batch_normalization(fc2)\n",
    "    \n",
    "    # layer 6 -> 7 \n",
    "    fc3 = tf.contrib.layers.fully_connected(inputs=fc2, num_outputs=512, activation_fn=tf.nn.relu)\n",
    "    fc3 = tf.nn.dropout(fc3, keep_prob)\n",
    "    fc3 = tf.layers.batch_normalization(fc3)\n",
    "    \n",
    "    # layer 7 -> 8 \n",
    "    fc4 = tf.contrib.layers.fully_connected(inputs=fc3, num_outputs=1024, activation_fn=tf.nn.relu)\n",
    "    fc4 = tf.nn.dropout(fc4, keep_prob)\n",
    "    fc4 = tf.layers.batch_normalization(fc4)\n",
    "    \n",
    "    # layer 8 -> 9\n",
    "    out = tf.contrib.layers.fully_connected(fc4, num_outputs=10, activation_fn=None)\n",
    "    \n",
    "    return out\n",
    "\n",
    "# hyper params\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "keep_probability = 0.7\n",
    "\n",
    "cifar10_dataset_folder_path = '/home/ed/Documents/code/fed_lrn/cifar-10-batches-py'\n",
    "\n",
    "logits = simple_conv(x, keep_prob)\n",
    "model = tf.identity(logits, name='logits')\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "    x = (x - min_val) / (max_val - min_val)\n",
    "    return x\n",
    "    \n",
    "def one_hot_encode(x):\n",
    "    encoded = np.zeros((len(x), 10))\n",
    "    \n",
    "    for idx, val in enumerate(x):\n",
    "        encoded[idx][val] = 1\n",
    "    return encoded\n",
    "\n",
    "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
    "    features = normalize(features)\n",
    "    labels = one_hot_encode(labels)\n",
    "    \n",
    "    pickle.dump((features, labels), open(filename, 'wb'))\n",
    "    \n",
    "def preprocess_and_save_data(cifar10_fs_path, normalize, one_hot_encode):\n",
    "    n_batches = 5\n",
    "    valid_features = []\n",
    "    valid_labels = []\n",
    "    \n",
    "    for batch_i in range(1, (n_batches + 1)):\n",
    "        features, labels = load_cfar10_batch('/home/ed/Documents/code/fed_lrn/cifar-10-batches-py', batch_i)\n",
    "    \n",
    "        index_of_validation = int(len(features) * 0.1)\n",
    "\n",
    "        _preprocess_and_save(normalize, one_hot_encode,\n",
    "                             features[:-index_of_validation], labels[:-index_of_validation],\n",
    "                             'preprocess_batch_' + str(batch_i) + '.p')\n",
    "\n",
    "        valid_features.extend(features[-index_of_validation:])\n",
    "        valid_labels.extend(labels[-index_of_validation:])\n",
    "    \n",
    "    _preprocess_and_save(normalize, one_hot_encode,\n",
    "                        np.array(valid_features), np.array(valid_labels),\n",
    "                        'preprocess_validation.p')\n",
    "    \n",
    "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "    \n",
    "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    test_labels = batch['labels']\n",
    "    \n",
    "    _preprocess_and_save(normalize, one_hot_encode,\n",
    "                        np.array(test_features), np.array(test_labels),\n",
    "                        'preprocess_training.p')\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_model(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\" \n",
    "    Method to train model outlined above\n",
    "    \"\"\"\n",
    "    session.run(optimizer,\n",
    "               feed_dict={\n",
    "                   x: feature_batch,\n",
    "                   y: label_batch,\n",
    "                   keep_prob: keep_probability,\n",
    "               })\n",
    "\n",
    "    \n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    loss = sess.run(cost, \n",
    "                    feed_dict={\n",
    "                        x: feature_batch,\n",
    "                        y: label_batch,\n",
    "                        keep_prob: 1.\n",
    "                    })\n",
    "    valid_acc = sess.run(accuracy, \n",
    "                         feed_dict={\n",
    "                             x: valid_features,\n",
    "                             y: valid_labels,\n",
    "                             keep_prob: 1.\n",
    "                         })\n",
    "    \n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))\n",
    "    \n",
    "def batch_data(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the data using a generator approach\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start: end]\n",
    "\n",
    "def load_preprocess_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "    Load the preprocessed data from disc -- CIFAR10/100 can be found at the URL below :\n",
    "    \n",
    "    https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    return batch_data(features, labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save_data('/home/ed/Documents/code/fed_lrn/cifar-10-batches-py', normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.0996 Validation Accuracy: 0.277000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     1.7659 Validation Accuracy: 0.340400\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.1903 Validation Accuracy: 0.458800\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.2540 Validation Accuracy: 0.512800\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.1441 Validation Accuracy: 0.558200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.0820 Validation Accuracy: 0.595000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     0.8814 Validation Accuracy: 0.594200\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     0.7792 Validation Accuracy: 0.629600\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     0.8596 Validation Accuracy: 0.632800\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     0.6251 Validation Accuracy: 0.661800\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     0.7826 Validation Accuracy: 0.671400\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     0.6416 Validation Accuracy: 0.690800\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     0.5097 Validation Accuracy: 0.681400\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     0.6076 Validation Accuracy: 0.693800\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     0.4414 Validation Accuracy: 0.710200\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     0.4646 Validation Accuracy: 0.687400\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     0.4285 Validation Accuracy: 0.697800\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     0.3907 Validation Accuracy: 0.701400\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     0.4537 Validation Accuracy: 0.709400\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     0.2990 Validation Accuracy: 0.710400\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     0.3428 Validation Accuracy: 0.724200\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     0.3469 Validation Accuracy: 0.709600\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     0.2580 Validation Accuracy: 0.715200\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     0.3310 Validation Accuracy: 0.719800\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     0.2163 Validation Accuracy: 0.729200\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     0.2129 Validation Accuracy: 0.723200\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     0.2496 Validation Accuracy: 0.694800\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     0.1856 Validation Accuracy: 0.731600\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     0.2841 Validation Accuracy: 0.722000\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     0.2112 Validation Accuracy: 0.727800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     0.1889 Validation Accuracy: 0.732400\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     0.2055 Validation Accuracy: 0.715600\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.1357 Validation Accuracy: 0.736000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     0.1651 Validation Accuracy: 0.726600\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     0.1428 Validation Accuracy: 0.740400\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.1724 Validation Accuracy: 0.739600\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     0.1154 Validation Accuracy: 0.745400\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.1137 Validation Accuracy: 0.747200\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.1687 Validation Accuracy: 0.728800\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     0.1265 Validation Accuracy: 0.719400\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.0772 Validation Accuracy: 0.747200\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.1035 Validation Accuracy: 0.733200\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.0807 Validation Accuracy: 0.748800\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.1028 Validation Accuracy: 0.731000\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.1010 Validation Accuracy: 0.735200\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.0667 Validation Accuracy: 0.740800\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     0.0905 Validation Accuracy: 0.743200\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.0421 Validation Accuracy: 0.756200\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.1210 Validation Accuracy: 0.746000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.0934 Validation Accuracy: 0.737000\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "save_model_path = './saved_model'\n",
    "print(\"Training...\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # intialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        n_batches = 5\n",
    "        \n",
    "        for batch_i in range(1, n_batches+1):\n",
    "            for batch_features, batch_labels in load_preprocess_batch(batch_i, batch_size):\n",
    "                train_model(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            \n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "        \n",
    "        # Save Model\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
